{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e8318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\administrateur\\Downloads\\tweets-data.csv\")\n",
    "\n",
    "# Sample 500 rows (for reproducibility, use random_state)\n",
    "df_sample = df.sample(n=500, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d630daae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Date Created', 'Number of Likes', 'Source of Tweet',\n",
      "       'Tweets', 'hashtag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_sample.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6b3e8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\administrateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\administrateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_tweet(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)                # Remove mentions and hashtags\n",
    "    text = re.sub(r'\\d+', '', text)                      # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)                  # Remove punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = [w for w in tokens if w not in stop_words]\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfc785dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[\"Tweets\"] = df_sample[\"Tweets\"].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5040441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\administrateur\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    scores = sia.polarity_scores(text)\n",
    "    compound = scores['compound']\n",
    "    if compound >= 0.05:\n",
    "        sentiment = \"positive\"\n",
    "    elif compound <= -0.05:\n",
    "        sentiment = \"negative\"\n",
    "    else:\n",
    "        sentiment = \"neutral\"\n",
    "    return pd.Series([sentiment, compound])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ba348aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[[\"sentiment_label\", \"sentiment_score\"]] = df_sample[\"Tweets\"].apply(get_sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dd1b75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Tweets sentiment_label  \\\n",
      "0  le de sanaga ls sont morts comme ils ont vÃ©cu ...         neutral   \n",
      "1                                                            neutral   \n",
      "2                                  exclusive content        positive   \n",
      "3  auch heute geht die politische nachricht des t...        negative   \n",
      "4  type would take homemade playstationcontrolled...         neutral   \n",
      "5                                                            neutral   \n",
      "6  mishap incredible force amp speed crushing wat...        negative   \n",
      "7                                                            neutral   \n",
      "8                                   le le retour via         neutral   \n",
      "9  il segretario di stato americano non credo che...         neutral   \n",
      "\n",
      "   sentiment_score  \n",
      "0           0.0000  \n",
      "1           0.0000  \n",
      "2           0.1280  \n",
      "3          -0.5994  \n",
      "4           0.0000  \n",
      "5           0.0000  \n",
      "6          -0.5859  \n",
      "7           0.0000  \n",
      "8           0.0000  \n",
      "9           0.0000  \n"
     ]
    }
   ],
   "source": [
    "print(df_sample[[\"Tweets\", \"sentiment_label\", \"sentiment_score\"]].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
